[["index.html", "Supplementary material for Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Section 1 Introduction 1.1 Attribution", " Supplementary material for Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Vijay Ramesh Pratik R. Gupte Morgan W. Tingley VV Robin Ruth DeFries 2020-12-25 Section 1 Introduction This is supplementary material for a manuscript that uses citizen science data to model the occupancy of birds in the southern Western Ghats, India. The main project can be found here: https://github.com/pratikunterwegs/eBirdOccupancy. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen "],["selecting-species-of-interest.html", "Section 2 Selecting species of interest 2.1 Prepare libraries 2.2 Read species of interest 2.3 Load raw data for locations 2.4 Get proportional obs counts in 25km cells 2.5 Which species are reported sufficiently in checklists? 2.6 Figure: Checklist distribution 2.7 Prepare the species list", " Section 2 Selecting species of interest This script shows the proportion of checklists that report a particular species across every 25km by 25km grid across the Nilgiris and the Anamalais. Using this analysis, we arrived at a final list of species for occupancy modeling. We derived this list from inclusion criteria adapted from the State of Indiaâ€™s Birds 2020 (Viswanathan et al., 2020). Initially, we considered all 561 species in eBird that occurred within the outlines of our study area. We then considered only those species that had a minimum of 1000 detections each between 2013 and 2019 (reducing to 303 species). Next, the study area was divided into 25 x 25 km cells following (Viswanathan et al., 2020). We then kept only those species that occurred in at least 5% of all checklists across 50% of the 25 x 25 km cells from where they have been reported (reducing to 93 species). We used the above criteria to ensure as much uniform sampling of a species as possible across our study area and to reduce any erroneous associations between environmental drivers and species occupancy. Across our final list of 93 species, we analyzed a total of ~3.2 million detections (presences) between 2013 and 2019. 2.1 Prepare libraries # load libraries library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(readr) library(ggplot2) library(ggthemes) library(scico) # round any function round_any &lt;- function(x, accuracy = 25000) { round(x / accuracy) * accuracy } 2.2 Read species of interest We initally considered all species that # add species of interest specieslist &lt;- read.csv(&quot;data/01_list-all-spp-byCount.csv&quot;) speciesOfInterest &lt;- specieslist$scientific_name 2.3 Load raw data for locations # read in shapefile of the study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) box &lt;- st_bbox(wg) # read in data and subset # To access the latest dataset, please visit: https://ebird.org/data/download and set the file path accordingly. ebd &lt;- fread(&quot;data/ebd_IN_relApr-2020.txt&quot;) ebd &lt;- ebd[between(LONGITUDE, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(LATITUDE, box[&quot;ymin&quot;], box[&quot;ymax&quot;]), ] ebd &lt;- ebd[year(`OBSERVATION DATE`) &gt;= 2013, ] # make new column names newNames &lt;- str_replace_all(colnames(ebd), &quot; &quot;, &quot;_&quot;) %&gt;% str_to_lower() setnames(ebd, newNames) # keep useful columns columnsOfInterest &lt;- c( &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;sampling_event_identifier&quot; ) ebd &lt;- ebd[, ..columnsOfInterest] Add a spatial filter and assign grids of 25km x 25km. # strict spatial filter and assign grid locs &lt;- ebd[, .(longitude, latitude)] # transform to UTM and get 20km boxes coords &lt;- setDF(locs) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # convert wg to UTM for filter wg &lt;- st_transform(wg, 32643) coords &lt;- coords %&gt;% filter(id %in% unlist(st_contains(wg, coords))) %&gt;% rename(longitude = X, latitude = Y) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% as.data.table() # remove unneeded objects rm(locs) gc() coords &lt;- coords[, .N, by = .(longitude, latitude, X, Y)] ebd &lt;- merge(ebd, coords, all = FALSE, by = c(&quot;longitude&quot;, &quot;latitude&quot;)) ebd &lt;- ebd[(longitude %in% coords$longitude) &amp; (latitude %in% coords$latitude), ] 2.4 Get proportional obs counts in 25km cells # round to 25km cell in UTM coords ebd[, `:=`(X = round_any(X), Y = round_any(Y))] # count checklists in cell ebd_summary &lt;- ebd[, nchk := length(unique(sampling_event_identifier)), by = .(X, Y) ] # count checklists reporting each species in cell and get proportion ebd_summary &lt;- ebd_summary[, .(nrep = length(unique( sampling_event_identifier ))), by = .(X, Y, nchk, scientific_name) ] ebd_summary[, p_rep := nrep / nchk] # filter for soi ebd_summary &lt;- ebd_summary[scientific_name %in% speciesOfInterest, ] # complete the dataframe for no reports # keep no reports as NA --- allows filtering based on proportion reporting ebd_summary &lt;- setDF(ebd_summary) %&gt;% complete( nesting(X, Y), scientific_name # , # fill = list(p_rep = 0) ) %&gt;% filter(!is.na(p_rep)) 2.5 Which species are reported sufficiently in checklists? # A total of 42 unique grids (of 25km by 25km) across the study area # total number of checklists across unique grids tot_n_chklist &lt;- ebd_summary %&gt;% distinct(X, Y, nchk) # species-specific number of grids spp_grids &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% distinct(X, Y) %&gt;% count(scientific_name, name = &quot;n_grids&quot; ) # Write the above two results write_csv(tot_n_chklist, &quot;data/nchk_per_grid.csv&quot;) write_csv(spp_grids, &quot;data/ngrids_per_spp.csv&quot;) # left-join the datasets ebd_summary &lt;- left_join(ebd_summary, spp_grids, by = &quot;scientific_name&quot;) # check the proportion of grids across which this cut-off is met for each species # Is it &gt; 90% or 70%? # For example, with a 3% cut-off, ~100 species are occurring in &gt;50% # of the grids they have been reported in p_cutoff &lt;- 0.05 # Proportion of checklists a species has been reported in grid_proportions &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% tally(p_rep &gt;= p_cutoff) %&gt;% mutate(prop_grids_cut = n / (spp_grids$n_grids)) %&gt;% arrange(desc(prop_grids_cut)) grid_prop_cut &lt;- filter( grid_proportions, prop_grids_cut &gt; p_cutoff ) # Write the results write_csv(grid_prop_cut, &quot;data/chk_5_percent.csv&quot;) # Identifying the number of species that occur in potentially &lt;5% of all lists total_number_lists &lt;- sum(tot_n_chklist$nchk) spp_sum_chk &lt;- ebd_summary %&gt;% distinct(X, Y, scientific_name, nrep) %&gt;% group_by(scientific_name) %&gt;% mutate(sum_chk = sum(nrep)) %&gt;% distinct(scientific_name, sum_chk) # Approximately 90 to 100 species occur in &gt;5% of all checklists prop_all_lists &lt;- spp_sum_chk %&gt;% mutate(prop_lists = sum_chk / total_number_lists) %&gt;% arrange(desc(prop_lists)) 2.6 Figure: Checklist distribution # add land library(rnaturalearth) land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) # crop land land &lt;- st_transform(land, 32643) Proportion of checklists reporting a species in each grid cell (25km side) between 2013 and 2019. Checklists were filtered to be within the boundaries of the Nilgiris and the Anamalai hills (black outline), but rounding to 25km cells may place cells outside the boundary. Deeper shades of red indicate a higher proportion of checklists reporting a species. 2.7 Prepare the species list # write the new list of species that occur in at least 5% of checklists across a minimum of 50% of the grids they have been reported in new_sp_list &lt;- semi_join(specieslist, grid_prop_cut, by = &quot;scientific_name&quot;) write_csv(new_sp_list, &quot;data/03_list-of-species-cutoff.csv&quot;) "],["landcover-classification.html", "Section 3 Landcover classification", " Section 3 Landcover classification This script was used to classify a 2019 Sentinel composite image across the Nilgiris and the Anamalais into seven distinct land cover types. The same code can be viewed on GEE here: https://code.earthengine.google.com/ec69fc4ffad32a532b25202009243d42. // Data: Groundtruthed points from Arasumani et al 2019 // Function to obtain a Cloud-Free Image // /** * Function to mask clouds using the Sentinel-2 QA band * @param {ee.Image} image Sentinel-2 image * @return {ee.Image} cloud masked Sentinel-2 image */ function maskS2clouds(image) { var qa = image.select(&#39;QA60&#39;); // Bits 10 and 11 are clouds and cirrus, respectively. var cloudBitMask = 1 &lt;&lt; 10; var cirrusBitMask = 1 &lt;&lt; 11; // Both flags should be set to zero, indicating clear conditions. var mask = qa.bitwiseAnd(cloudBitMask).eq(0) .and(qa.bitwiseAnd(cirrusBitMask).eq(0)); return image.updateMask(mask).divide(10000); } // Importing shapefile needed for classification var clipper = function(image){ return image.clip(WG_Buffer); }; // Import raw Sentinel scenes and clip them over your study area var filtered = sentinel.filterDate(&#39;2018-01-01&#39;,&#39;2018-12-01&#39;).map(clipper); // Load Sentinel-2 TOA reflectance data. // Pre-filter to get less cloudy granules. var dataset = filtered.filter(ee.Filter.lt(&#39;CLOUDY_PIXEL_PERCENTAGE&#39;, 20)) .map(maskS2clouds); var scene = dataset.reduce(ee.Reducer.median()); Map.addLayer(WG_Buffer, {}, &#39;Buffer Outline for Nil/Ana/Pal&#39;); // Map.addLayer(scene,{},&#39;Image for Classification&#39;); // Map.addLayer(WG, {},&#39;Outline for Nilgiris/Anaimalais/Palanis&#39;); // Step 2: Creating training data manually // Added a new shapefile field manually in ArcMap so that GEE can take a float field for classification // Field: landcover // Values: agriculture (1), forest (2), grassland (3), plantation (4), settlements (5), tea (6), waterbodies (7) // Note - Arasu has classified plantation as Acacia, Pine et al sub classes (for future analysis) // Merging the featureCollections to obtain a single featureCollection var trainingFeatures = agriculture.merge(forests).merge(forests2).merge(grasslands).merge(grasslands2) .merge(settlements).merge(plantations) .merge(waterbodies).merge(tea).merge(tea2).merge(tea3).merge(forests3); // // Specify the bands of the sentinel image to be used as predictors (p) var predictionBands = [&#39;B2_median&#39;,&#39;B3_median&#39;,&#39;B4_median&#39;,&#39;B8_median&#39;]; // // Now a random forest is a collection of random trees. It&#39;s predictions are used to compute an // // average (regression) or vote on a label (classification) var sample = scene.select(predictionBands) .sampleRegions({ collection: trainingFeatures, properties : [&#39;landcover&#39;], scale: 10 }); // Let&#39;s run a classifier for randomForest var classifier = ee.Classifier.randomForest(10).train({ features: sample, classProperty: &#39;landcover&#39;, inputProperties: predictionBands }); var classified = scene.select(predictionBands).classify(classifier); Map.addLayer(classified, {min:1, max:7,palette:[ &#39;be4fc4&#39;, // agriculture, violetish &#39;04a310&#39;, // forests, lighter green &#39;cbb315&#39;, // grasslands, yellowish &#39;c17111&#39;, // plantations, brownish &#39;b0a69d&#39;, // settlements, grayish &#39;025a05&#39;, // tea, dark greenish &#39;2035df&#39;, // waterbodies, royal blue ]}, &#39;classified&#39;); // Partitioning training data to run an accuracy assessment // Adding a randomColumn of values ranging from 0 to 1 var trainingTesting = sample.randomColumn(); var trainingSet = trainingTesting.filter(ee.Filter.lt(&#39;random&#39;,0.8)); var testingSet = trainingTesting.filter(ee.Filter.gte(&#39;random&#39;,0.2)); // Now run the classifier only with the trainingSet var trained = ee.Classifier.randomForest(10).train({ features: trainingSet, classProperty: &#39;landcover&#39;, inputProperties: predictionBands }); // Now classify the testData and obtain a Confusion matrix var confusionMatrix = ee.ConfusionMatrix(testingSet.classify(trained) .errorMatrix({ actual: &#39;landcover&#39;, predicted: &#39;classification&#39; })); // Now print the ConfusionMatrix and expand the object to inspect the matrix() // The entries represent the number of pixels and the items on the diagonal represent // correct classification. Items off the diagonal are misclassifications, where class in row i // is classified as column j // One can also obtain basic descriptive statistics from the confusionMatrix // Note this won&#39;t work as the number of pixels is too high (Export as .csv to obtain result) // print(&#39;Confusion matrix:&#39;, confusionMatrix); // print(&#39;Overall Accuracy:&#39;, confusionMatrix.accuracy()); // print(&#39;Producers Accuracy:&#39;, confusionMatrix.producersAccuracy()); // print(&#39;Consumers Accuracy:&#39;, confusionMatrix.consumersAccuracy()); // Since printing the above is gives you a computation timed out error var exportconfusionMatrix = ee.Feature(null, {matrix: confusionMatrix.array()}); var exportAccuracy = ee.Feature(null, {matrix: confusionMatrix.accuracy()}); Export.table.toDrive({ collection: ee.FeatureCollection(exportconfusionMatrix), description: &#39;confusionMatrix&#39;, fileFormat: &#39;CSV&#39; }); Export.table.toDrive({ collection: ee.FeatureCollection(exportAccuracy), description: &#39;Accuracy&#39;, fileFormat: &#39;CSV&#39; }); // Below code suggests that the current projection system is WGS84 // print(classified.projection()); // To project it to UTM var reprojected = classified.reproject(&#39;EPSG:32643&#39;,null,10); // Export classified image Export.image.toDrive({ image: classified, description: &#39;Classified Image&#39;, scale: 10, region: WG_Buffer, //.geometry().bounds(), fileFormat: &#39;GeoTIFF&#39;, formatOptions: { cloudOptimized: true }, maxPixels: 618539476 }); // Export projected image Export.image.toDrive({ image: reprojected, description: &#39;Reprojected Image&#39;, scale: 10, region: WG_Buffer, //.geometry().bounds(), fileFormat: &#39;GeoTIFF&#39;, formatOptions: { cloudOptimized: true }, maxPixels: 618539476 }); "],["spatial-autocorrelation-of-climatic-predictors.html", "Section 4 Spatial Autocorrelation of Climatic Predictors 4.1 Load libraries 4.2 Prepare data 4.3 Calculate variograms of environmental layers 4.4 Visualise variograms of environmental data", " Section 4 Spatial Autocorrelation of Climatic Predictors 4.1 Load libraries # load libs library(raster) library(gstat) library(stars) library(purrr) library(tibble) library(dplyr) library(tidyr) library(glue) library(scales) library(gdalUtils) library(sf) # plot libs library(ggplot2) library(ggthemes) library(scico) library(gridExtra) library(cowplot) library(ggspatial) #&#39; make custom functiont to convert matrix to df raster_to_df &lt;- function(inp) { # assert is a raster obj assertthat::assert_that(&quot;RasterLayer&quot; %in% class(inp), msg = &quot;input is not a raster&quot; ) coords &lt;- coordinates(inp) vals &lt;- getValues(inp) data &lt;- tibble(x = coords[, 1], y = coords[, 2], value = vals) return(data) } 4.2 Prepare data # list landscape covariate stacks landscape_files &lt;- &quot;data/spatial/landscape_resamp01km.tif&quot; landscape_data &lt;- stack(landscape_files) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio_01&quot;, &quot;bio_12&quot;) names(landscape_data) &lt;- c(elev_names, chelsa_names, &quot;landcover&quot;) # get chelsa rasters chelsa &lt;- landscape_data[[chelsa_names]] chelsa &lt;- purrr::map(as.list(chelsa), raster_to_df) 4.3 Calculate variograms of environmental layers # prep variograms vgrams &lt;- purrr::map(chelsa, function(z) { z &lt;- drop_na(z) vgram &lt;- gstat::variogram(value ~ 1, loc = ~ x + y, data = z) return(vgram) }) # save temp save(vgrams, file = &quot;data/chelsa/chelsaVariograms.rdata&quot;) # get variogram data vgrams &lt;- purrr::map(vgrams, function(df) { df %&gt;% select(dist, gamma) }) vgrams &lt;- tibble( variable = chelsa_names, data = vgrams ) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) # Plot library(rnaturalearth) land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) # crop land land &lt;- st_transform(land, 32643) 4.4 Visualise variograms of environmental data # make ggplot of variograms yaxis &lt;- c(&quot;semivariance&quot;, &quot;&quot;) xaxis &lt;- c(&quot;&quot;, &quot;distance (km)&quot;) fig_vgrams &lt;- purrr::pmap(list(vgrams$data, yaxis, xaxis), function(df, ya, xa) { ggplot(df) + geom_line(aes(x = dist / 1000, y = gamma), size = 0.2, col = &quot;grey&quot;) + geom_point(aes(x = dist / 1000, y = gamma), col = &quot;black&quot;) + scale_x_continuous(labels = comma, breaks = c(seq(0, 100, 25))) + scale_y_log10(labels = comma) + labs(x = xa, y = ya) + theme_few() + theme( axis.text.y = element_text(angle = 90, hjust = 0.5, size = 8), strip.text = element_blank() ) }) # fig_vgrams &lt;- purrr::map(fig_vgrams, ggplot2::ggplotGrob) # make ggplot of chelsa data chelsa &lt;- as.list(landscape_data[[chelsa_names]]) %&gt;% purrr::map(stars::st_as_stars) # colour palettes pal &lt;- c(&quot;bilbao&quot;, &quot;davos&quot;) title &lt;- c( &quot;a Annual Mean Temperature&quot;, &quot;b Annual Precipitation&quot; ) direction &lt;- c(1, 1) lims &lt;- list( range(values(landscape_data$bio_01), na.rm = T), range(values(landscape_data$bio_12), na.rm = T) ) fig_list_chelsa &lt;- purrr::pmap( list(chelsa, pal, title, direction, lims), function(df, pal, t, d, l) { ggplot() + stars::geom_stars(data = df) + geom_sf(data = land, fill = NA, colour = &quot;black&quot;) + geom_sf(data = wg, fill = NA, colour = &quot;black&quot;, size = 0.3) + scale_fill_scico( palette = pal, direction = d, label = comma, na.value = NA, limits = l ) + coord_sf( xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)] ) + ggspatial::annotation_scale(location = &quot;tr&quot;, width_hint = 0.4, text_cex = 1) + theme_few() + theme( legend.position = &quot;top&quot;, title = element_text(face = &quot;bold&quot;, size = 8), legend.key.height = unit(0.2, &quot;cm&quot;), legend.key.width = unit(1, &quot;cm&quot;), legend.text = element_text(size = 8), axis.title = element_blank(), axis.text.y = element_text(angle = 90, hjust = 0.5), panel.background = element_rect(fill = &quot;lightblue&quot;), legend.title = element_blank() ) + labs(x = NULL, y = NULL, title = t) } ) #fig_list_chelsa &lt;- purrr::map(fig_list_chelsa, ggplotGrob) # fig_list_chelsa &lt;- append(fig_list_chelsa, fig_vgrams) # lmatrix &lt;- matrix(c(c(1, 2, 3, 4, 5), c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)), # nrow = 3, byrow = T # ) # plot_grid &lt;- grid.arrange(grobs = fig_list_chelsa, layout_matrix = lmatrix) # # ggsave( # plot = plot_grid, filename = &quot;figs/fig_chelsa_variograms.png&quot;, # dpi = 300, width = 12, height = 6 # ) # dev.off() library(patchwork) fig_variogram &lt;- wrap_plots(append(fig_list_chelsa, fig_vgrams)) ggsave(fig_variogram, filename = &quot;figs/fig_chelsa_variograms.png&quot;, dpi = 300, width = 6, height = 6 ) CHELSA rasters with study area outline, and associated semivariograms. Semivariograms are on a log-transformed y-axis. "],["climatic-raster-resampling.html", "Section 5 Climatic raster resampling 5.1 Prepare landcover 5.2 Prepare spatial extent 5.3 Prepare CHELSA rasters 5.4 Resample prepared rasters", " Section 5 Climatic raster resampling 5.1 Prepare landcover To access the classified Sentinel image, please visit: https://code.earthengine.google.com/ec69fc4ffad32a532b25202009243d42 # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/classifiedImage-UTM.tif&quot; # get extent e &lt;- bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- map(c(100, 250, 500, 1e3, 2.5e3), function(x) { x * res_init }) # use gdalutils gdalwarp for resampling transform # to 1km from 10m for (i in 1:length(res_final)) { this_res &lt;- res_final[[i]] this_res_char &lt;- stringr::str_pad(this_res[1], 5, pad = &quot;0&quot;) gdalUtils::gdalwarp( srcfile = landcover, dstfile = as.character(glue(&quot;data/landUseClassification/lc_{this_res_char}m.tif&quot;)), tr = c(this_res), r = &quot;mode&quot;, te = c(e) ) } # read in resampled landcover raster files as a list lc_files &lt;- list.files(&quot;data/landUseClassification/&quot;, pattern = &quot;lc&quot;, full.names = TRUE) lc_data &lt;- map(lc_files, raster) 5.2 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) bbox &lt;- st_bbox(hills) 5.3 Prepare CHELSA rasters Please download the CHELSA rasters from https://chelsa-climate.org/bioclim/ # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr) { a &lt;- raster(chr) crs(a) &lt;- crs(buffer) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) names(chelsaData) &lt;- c(&quot;chelsa_bio10_01&quot;, &quot;chelsa_bio10_12&quot;) 5.4 Resample prepared rasters # make resampled data resamp_data &lt;- map(lc_data, function(this_scale) { rr &lt;- projectRaster( from = chelsaData, to = this_scale, crs = crs(this_scale), res = res(this_scale) ) }) # make a stars list resamp_data &lt;- map2(resamp_data, lc_data, function(z1, z2) { z2[z2 == 0] &lt;- NA z2 &lt;- append(z2, as.list(z1)) %&gt;% map(stars::st_as_stars) }) %&gt;% flatten() "],["climate-in-relation-to-landcover.html", "Section 6 Climate in Relation to Landcover 6.1 Prepare libraries 6.2 Prepare environmental data 6.3 Climatic variables over landcover", " Section 6 Climate in Relation to Landcover This script showcases how climatic predictors vary as a function of land cover types across our study area. 6.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) # plotting options library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = T) / sqrt(length(x)) } 6.2 Prepare environmental data # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio_01&quot;, &quot;bio_12&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;landcover&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, raster::getValues) names(land_data) &lt;- c(&quot;landcover&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% filter(landcover != 0) %&gt;% pivot_longer( cols = contains(&quot;bio&quot;), names_to = &quot;clim_var&quot; ) # %&gt;% # group_by(landcover, clim_var) %&gt;% # summarise_all(.funs = list(~mean(.), ~ci(.))) 6.3 Climatic variables over landcover Figure code is hidden in versions rendered as HTML and PDF. CHELSA climatic variables as a function of landcover class. Grey points in the background represent raw data. "],["distribution-of-observer-expertise.html", "Section 7 Distribution of Observer Expertise 7.1 Prepare libraries 7.2 Load observer expertise scores and checklist covariates 7.3 Species observed in relation to observer expertise 7.4 Observer expertise in relation to landcover", " Section 7 Distribution of Observer Expertise This script plots observer expertise over time (2013-2019) as well as across land cover types. 7.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) library(readr) library(scales) # plotting libs library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = T) / sqrt(length(x)) } 7.2 Load observer expertise scores and checklist covariates # read in scores and checklist data and link scores &lt;- read_csv(&quot;data/03_data-obsExpertise-score.csv&quot;) data &lt;- read_csv(&quot;data/03_data-covars-perChklist.csv&quot;) data &lt;- left_join(data, scores, by = c(&quot;observer&quot; = &quot;observer&quot;)) data &lt;- dplyr::select(data, score, nSp, nSoi, landcover, year) %&gt;% filter(!is.na(score)) 7.3 Species observed in relation to observer expertise # summarise data by rounded score and year data_summary01 &lt;- data %&gt;% mutate(score = plyr::round_any(score, 0.2)) %&gt;% dplyr::select(score, year, nSp, nSoi) %&gt;% pivot_longer( cols = c(&quot;nSp&quot;, &quot;nSoi&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot; ) %&gt;% group_by(score, year, variable) %&gt;% summarise_at(vars(value), list(~ mean(.), ~ ci(.))) # make plot and export fig_nsp_score &lt;- ggplot(data_summary01) + geom_jitter( data = data, aes(x = score, y = nSp), col = &quot;grey&quot;, alpha = 0.2, size = 0.1 ) + geom_pointrange(aes( x = score, y = mean, ymin = mean - ci, ymax = mean + ci, col = as.factor(variable) ), position = position_dodge(width = 0.05) ) + facet_wrap(~year) + scale_y_log10() + # coord_cartesian(ylim=c(0,50))+ scale_colour_scico_d(palette = &quot;cork&quot;, begin = 0.2, end = 0.8) + labs(x = &quot;CCI&quot;, y = &quot;Number of Species Reported&quot;) + theme_few() + theme(legend.position = &quot;none&quot;) # export figure ggsave(filename = &quot;figs/fig_nsp_score.png&quot;, width = 12, height = 7, device = png(), dpi = 300) dev.off() Total number of species (blue) and species of interest to this study (green) reported in checklists from the study area over the years 2013 â€“ 2018, as a function of the expertise score of the reporting observer. Points represent means, with bars showing the 95% confidence intervals; data shown are for expertise scores rounded to multiples of 0.2, and the y-axis is on a log scale. Raw data are shown in the background (grey points). 7.4 Observer expertise in relation to landcover Figure code is hidden in versions rendered as HTML or PDF. Distribution of expertise scores in the seven landcover classes present in the study site. "],["matching-effort-cutoffs-with-spatial-independence-criteria.html", "Section 8 Matching Effort Cutoffs with Spatial Independence Criteria 8.1 Load librarires 8.2 Load data 8.3 Visualise limiting effort by spatial independence limits", " Section 8 Matching Effort Cutoffs with Spatial Independence Criteria How many sites would be lost if effort distance was restricted based on spatial independence? 8.1 Load librarires # load data packagaes library(data.table) library(dplyr) # load plotting packages library(ggplot2) library(scico) library(ggthemes) library(scales) 8.2 Load data # load checklist covariates data &lt;- fread(&quot;data/03_data-covars-perChklist.csv&quot;) effort_distance_summary &lt;- data[, effort_distance_class := cut(distance, breaks = c( -1, 0.001, 0.1, 0.25, 0.5, 1, 2.5, 5, Inf ), ordered_result = T)][, .N, by = effort_distance_class ][ order(effort_distance_class) ] effort_distance_summary[ , prop_effort := cumsum(effort_distance_summary$N) / nrow(data) ] 8.3 Visualise limiting effort by spatial independence limits # plot effort distance class cumulative sum fig_dist_exclusion &lt;- ggplot(effort_distance_summary) + geom_point(aes(effort_distance_class, prop_effort), size = 3) + geom_path(aes(effort_distance_class, prop_effort, group = NA)) + # scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ scale_x_discrete(labels = c( &quot;stationary&quot;, &quot;100m&quot;, &quot;250m&quot;, &quot;500m&quot;, &quot;1 km&quot;, &quot;2.5 km&quot;, &quot;5 km&quot; )) + theme_few() + theme(panel.grid = element_line(size = 0.2, color = &quot;grey&quot;)) + labs(x = &quot;effort distance cutoff&quot;, y = &quot;proportion of checklists&quot;) ggsave( plot = fig_dist_exclusion, &quot;figs/fig_cutoff_effort.png&quot;, height = 6, width = 8, dpi = 300 ) dev.off() "],["spatial-thinning-a-brief-comparison-of-approaches.html", "Section 9 Spatial Thinning: A Brief Comparison of Approaches 9.1 Prepare libraries 9.2 Traditional grid-based thinning 9.3 Network-based thinning 9.4 Finding modularity in proximity networks 9.5 Process proximity networks in R 9.6 A function that removes sites 9.7 Removing non-independent sites 9.8 Measuring method fallibility 9.9 Prepare data for Python 9.10 Count props under threshold in Python 9.11 Plot metrics for different methods", " Section 9 Spatial Thinning: A Brief Comparison of Approaches 9.1 Prepare libraries # load libraries library(tidyverse) library(glue) library(readr) library(sf) # plotting library(ggthemes) library(scico) library(scales) # ci func ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = T) / sqrt(length(x)) } # load python libs here library(reticulate) # set python path use_python(&quot;/usr/bin/python3&quot;) 9.2 Traditional grid-based thinning # load the shapefile of the study area wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # get scales # load checklist data and select one per rounded 500m coordinates { data &lt;- read_csv(&quot;data/03_data-covars-perChklist.csv&quot;) %&gt;% count(longitude, latitude, name = &quot;tot_effort&quot;) # how many unique points n_all_points &lt;- nrow(data) d_all_effort &lt;- sum(data$tot_effort) # round to different scales scale &lt;- c(100, 250, 500, 1000) # group data by scale data &lt;- crossing(scale, data) %&gt;% group_by(scale) %&gt;% nest() %&gt;% ungroup() } # select one point per grid cell data &lt;- mutate(data, data = map2(scale, data, function(sc, df) { # transform the data df &lt;- df %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% mutate( coordId = 1:nrow(.), X_round = plyr::round_any(X, sc), Y_round = plyr::round_any(Y, sc) ) # make a grid grid &lt;- st_make_grid(wg, cellsize = sc) # which cell contains which points grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) rm(grid) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot; ) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) # get summary for max max_sites &lt;- points_max %&gt;% ungroup() %&gt;% summarise( prop_points = length(coordId) / n_all_points, prop_effort = sum(tot_effort) / d_all_effort ) %&gt;% pivot_longer( cols = everything(), names_to = &quot;variable&quot; ) # select a random point in each grid points_rand &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot; ) %&gt;% group_by(cell) %&gt;% sample_n(size = 1) # get summary for rand rand_sites &lt;- points_rand %&gt;% ungroup() %&gt;% summarise( prop_points = length(coordId) / n_all_points, prop_effort = sum(tot_effort) / d_all_effort ) %&gt;% pivot_longer( cols = everything(), names_to = &quot;variable&quot; ) df &lt;- tibble( grid_rand = list(rand_sites), grid_max = list(max_sites), points_rand = list(points_rand), points_max = list(points_max) ) })) # unnest data data &lt;- unnest(data, cols = data) # save summary as another object data_thin_trad &lt;- data %&gt;% select(-contains(&quot;points&quot;)) %&gt;% pivot_longer( cols = -contains(&quot;scale&quot;), names_to = &quot;method&quot;, values_to = &quot;somedata&quot; ) %&gt;% unnest(cols = somedata) # save points for later comparison points_thin_trad &lt;- data %&gt;% select(contains(&quot;points&quot;), scale) rm(data) 9.3 Network-based thinning Load python libraries. # import classic python libs import numpy as np import matplotlib.pyplot as plt # libs for dataframes import pandas as pd # network lib import networkx as nx # import libs for geodata import geopandas as gpd # import ckdtree from scipy.spatial import cKDTree 9.4 Finding modularity in proximity networks # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/03_data-covars-perChklist.csv&quot;) ul = chkCovars[[&#39;longitude&#39;, &#39;latitude&#39;]].drop_duplicates(subset=[&#39;longitude&#39;, &#39;latitude&#39;]) ul[&#39;coordId&#39;] = np.arange(0, ul.shape[0]) # get effort at each coordinate effort = chkCovars.groupby([&#39;longitude&#39;, &#39;latitude&#39;]).size().to_frame(&#39;tot_effort&#39;) effort = effort.reset_index() # merge effort on ul ul = pd.merge(ul, effort, on=[&#39;longitude&#39;, &#39;latitude&#39;]) # make gpd and drop col from ul ulgpd = gpd.GeoDataFrame(ul, geometry=gpd.points_from_xy(ul.longitude, ul.latitude)) ulgpd.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 ulgpd = ulgpd.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) ul = pd.DataFrame(ul.drop(columns=&quot;geometry&quot;)) # function to use ckdtrees for nearest point finding def ckd_pairs(gdfA, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) ckd_tree = cKDTree(A) dist = ckd_tree.query_pairs(r=dist_indep, output_type=&#39;ndarray&#39;) return dist # define scales in metres scales = [100, 250, 500, 1000] # function to process ckd_pairs def make_modules(scale): site_pairs = ckd_pairs(gdfA=ulgpd, dist_indep=scale) site_pairs = pd.DataFrame(data=site_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) site_pairs[&#39;scale&#39;] = scale # get site ids site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) site_id = np.unique(site_id) # make network network = nx.from_pandas_edgelist(site_pairs, &#39;p1&#39;, &#39;p2&#39;) # get modules modules = list(nx.algorithms.community.greedy_modularity_communities(network)) # get modules as df m = [] for i in np.arange(len(modules)): module_number = [i] * len(modules[i]) module_coords = list(modules[i]) m = m + list(zip(module_number, module_coords)) # add location and summed sampling duration unique_locs = ul[ul.coordId.isin(site_id)] module_data = pd.DataFrame(m, columns=[&#39;module&#39;, &#39;coordId&#39;]) module_data = pd.merge(module_data, unique_locs, on=&#39;coordId&#39;) # add scale module_data[&#39;scale&#39;] = scale return [site_pairs, module_data] # run make modules on ulgpd at scales data = list(map(make_modules, scales)) # extract data for output tot_pair_data = [] tot_module_data = [] for i in np.arange(len(data)): tot_pair_data.append(data[i][0]) tot_module_data.append(data[i][1]) tot_pair_data = pd.concat(tot_pair_data, ignore_index=True) tot_module_data = pd.concat(tot_module_data, ignore_index=True) # make dict of positions and array of coordinates # site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) # site_id = np.unique(site_id) # locations_df = ul[ul.coordId.isin(site_id)][[&#39;longitude&#39;, &#39;latitude&#39;]].to_numpy() # pos_dict = dict(zip(site_id, locations_df)) # output data tot_module_data.to_csv(path_or_buf=&quot;data/site_modules.csv&quot;, index=False) tot_pair_data.to_csv(path_or_buf=&quot;data/site_pairs.csv&quot;, index=False) # ends here 9.5 Process proximity networks in R # read in pair and module data pairs &lt;- read_csv(&quot;data/site_pairs.csv&quot;) mods &lt;- read_csv(&quot;data/site_modules.csv&quot;) # count pairs at each scale count(pairs, scale) pairs %&gt;% group_by(scale) %&gt;% summarise(non_indep_pairs = length(unique(c(p1, p2))) / n_all_points) count(mods, scale) # nest by scale and add module data data &lt;- nest(pairs, data = c(p1, p2)) modules &lt;- group_by(mods, scale) %&gt;% nest() %&gt;% ungroup() # add module data data &lt;- mutate(data, modules = modules$data, data = map2(data, modules, function(df, m) { df &lt;- left_join(df, m, by = c(&quot;p1&quot; = &quot;coordId&quot;)) df &lt;- left_join(df, m, by = c(&quot;p2&quot; = &quot;coordId&quot;)) df &lt;- filter(df, module.x == module.y) return(df) }) ) %&gt;% select(-modules) # split by module data$data &lt;- map(data$data, function(df) { df &lt;- group_by(df, module.x, module.y) %&gt;% nest() %&gt;% ungroup() return(df) }) 9.6 A function that removes sites # a function to remove sites remove_which_sites &lt;- function(pair_data) { { a &lt;- pair_data %&gt;% select(p1, p2) nodes_a_init &lt;- unique(c(a$p1, a$p2)) i_n_d &lt;- filter(mods, coordId %in% nodes_a_init) %&gt;% select(node = coordId, tot_effort) %&gt;% mutate(s_f_r = NA) nodes_keep &lt;- c() nodes_removed &lt;- c() } while (nrow(a) &gt; 0) { # how many nodes in a nodes_a &lt;- unique(c(a$p1, a$p2)) # get node or site efforts and arrange in ascending order b &lt;- i_n_d %&gt;% filter(node %in% nodes_a) for (i in 1:nrow(b)) { # which node to remove node_out &lt;- b$node[i] # how much tot_effort lost d_n_o &lt;- b$tot_effort[i] # how many rows remain in a if node_out is removed? a_n_o &lt;- filter(a, p1 != node_out, p2 != node_out) indep_nodes &lt;- setdiff(nodes_a, unique(c(a_n_o$p1, a_n_o$p2, node_out))) # how much sampling effort made spatially independent indep_sampling &lt;- filter(b, node %in% indep_nodes) %&gt;% summarise(tot_effort = sum(tot_effort)) %&gt;% .$tot_effort # message(glue::glue(&#39;{node_out} removal frees {indep_sampling} m&#39;)) # sampling freed by sampling lost b$s_f_r[i] &lt;- indep_sampling / d_n_o } # arrange node data by decreasing sfr and increasing tot_effort # highest tot_effort nodes are processed last b &lt;- arrange(b, -s_f_r, tot_effort) nodes_removed &lt;- c(nodes_removed, b$node[1]) # remove pairs of nodes containing the highest sfr node in b a &lt;- filter(a, p1 != b$node[1], p2 != b$node[1]) nodes_keep &lt;- c(nodes_keep, setdiff(nodes_a, unique(c(a$p1, a$p2, nodes_removed)))) } message(glue::glue(&quot;keeping {length(nodes_keep)} of {length(nodes_a_init)}&quot;)) # node_status &lt;- tibble(nodes = c(nodes_keep, nodes_removed), # status = c(rep(TRUE, length(nodes_keep)), # rep(FALSE, length(nodes_removed)))) return(as.integer(nodes_removed)) } 9.7 Removing non-independent sites # remove 5km and 2.5km scale data &lt;- data %&gt;% filter(scale &lt;= 1000) # run select sites on the various modules sites_removed &lt;- map(data$data, function(df) { remove_sites &lt;- unlist(purrr::map(df$data, remove_which_sites)) }) # save as rdata save(sites_removed, file = &quot;data/data_network_sites_removed.rdata&quot;) # get python sites ul &lt;- py$ul load(&quot;data/data_network_sites_removed.rdata&quot;) # subset sites data &lt;- mutate(data, data = map(sites_removed, function(site_id) { as_tibble(filter(ul, !coordId %in% site_id)) }) ) # which points are kept points_thin_net &lt;- mutate(data, data = map(data, function(df) { df &lt;- df %&gt;% select(&quot;longitude&quot;, &quot;latitude&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() }) ) # get metrics for method data_thin_net &lt;- unnest(data, cols = &quot;data&quot;) %&gt;% group_by(scale) %&gt;% summarise( prop_points = length(coordId) / n_all_points, prop_effort = sum(tot_effort) / d_all_effort ) %&gt;% mutate(method = &quot;network&quot;) %&gt;% pivot_longer( cols = -one_of(c(&quot;method&quot;, &quot;scale&quot;)), names_to = &quot;variable&quot; ) 9.8 Measuring method fallibility How many points, at different spatial scales, remain after the application of each method? 9.9 Prepare data for Python # get points by each method points_list &lt;- append(points_thin_net$data, values = append( points_thin_trad$points_rand, points_thin_trad$points_max )) # get scales as list scales_list &lt;- list(100, 250, 500, 1000, rep(c(100, 250, 500, 1000), 2)) %&gt;% flatten() # send to python py$points_list &lt;- points_list py$scales_list &lt;- scales_list 9.10 Count props under threshold in Python # a function to convert to gpd def make_gpd(df): df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y)) df.crs = {&#39;init&#39; :&#39;epsg:32643&#39;} return df # function for mean nnd # function to use ckdtrees for nearest point finding def ckd_test(gdfA, gdfB, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate([np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) dist_diff = list(map(lambda x: x - dist_indep, dist)) mean_dist_diff = np.asarray(dist_diff).mean() return mean_dist_diff # apply to all data points_list = list(map(make_gpd, points_list)) # get nnb all data mean_dist_diff = list(map(ckd_test, points_list, points_list, scales_list)) 9.11 Plot metrics for different methods # combine the thinning metrics data data_plot &lt;- bind_rows(data_thin_net, data_thin_trad) # get data for mean distance data_thin_compare &lt;- tibble( scale = unlist(scales_list), method = c( rep(&quot;network&quot;, 4), rep(&quot;grid_rand&quot;, 4), rep(&quot;grid_max&quot;, 4) ), `mean NND - buffer (m)` = unlist(py$mean_dist_diff) ) %&gt;% pivot_longer( cols = &quot;mean NND - buffer (m)&quot;, names_to = &quot;variable&quot; ) # bind rows with other data data_plot &lt;- bind_rows(data_plot, data_thin_compare) # plot results fig_spatial_thinning &lt;- ggplot(data_plot) + geom_vline(xintercept = scale, lty = 3, colour = &quot;grey&quot;, lwd = 0.4) + geom_line(aes(x = scale, y = value, col = method)) + geom_point(aes(x = scale, y = value, col = method, shape = method)) + facet_wrap(~variable, scales = &quot;free&quot;) + scale_shape_manual(values = c(1, 2, 0)) + scale_x_continuous(breaks = scale) + scale_y_continuous() + scale_colour_scico_d(palette = &quot;batlow&quot;, begin = 0.2, end = 0.8) + theme_few() + theme(legend.position = &quot;top&quot;) + labs(x = &quot;buffer distance (m)&quot;) # save ggsave(fig_spatial_thinning, filename = &quot;figs/fig_spatial_thinning_02.png&quot;, width = 10, height = 4, dpi = 300 ) dev.off() "],["predicting-species-specific-occupancy.html", "Section 10 Predicting Species-specific Occupancy 10.1 Prepare libraries 10.2 Read data", " Section 10 Predicting Species-specific Occupancy This supplement plots species-specific probabilities of occupancy as a function of significant environmental predictors. 10.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) # plotting library(ggplot2) library(patchwork) 10.2 Read data # read data data &lt;- read_csv(&quot;data/results/data_occupancy_predictors.csv&quot;) # drop na data &lt;- select( data, -ci ) %&gt;% drop_na() %&gt;% nest(data = c(predictor, m_group, seq_x, mean, scale)) Figure code is hidden in versions rendered as HTML and PDF. Example output is shown below. Figure here "]]
