[
["index.html", "Supplementary material for: Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Section 1 Introduction 1.1 Attribution", " Supplementary material for: Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Vijay Ramesh, Pratik R Gupte, and Morgan Tingley 2020-07-25 Section 1 Introduction This is supplementary material for a project in preparation that models occupancy for birds in the Nilgiri hills. The main project can be found here: https://github.com/pratikunterwegs/eBirdOccupancy. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen Morgan Tingley (PI) "],
["distance-to-roads.html", "Section 2 Distance to roads 2.1 Prepare libraries 2.2 Prepare data for processing 2.3 Species specific nearest sites 2.4 Explicit spatial filter 2.5 Species specific filter 2.6 Plot histogram: distance to roads 2.7 Table: Distance to roads 2.8 Plot histogram: distance to nearest site 2.9 Plot species specific histograms: distance to nearest site 2.10 Table: Species specific nearest site 2.11 Plot map: points on roads", " Section 2 Distance to roads 2.1 Prepare libraries # load libraries library(reticulate) library(sf) library(dplyr) library(scales) library(readr) library(purrr) library(ggplot2) library(ggthemes) library(ggspatial) library(scico) # round any function round_any &lt;- function(x, accuracy = 20000){round(x/accuracy)*accuracy} # ci function ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))} # set python path use_python(&quot;/usr/bin/python3&quot;) Importing python libraries. # import classic python libs import itertools from operator import itemgetter import numpy as np import matplotlib.pyplot as plt import math # libs for dataframes import pandas as pd # import libs for geodata from shapely.ops import nearest_points import geopandas as gpd import rasterio # import ckdtree from scipy.spatial import cKDTree from shapely.geometry import Point, MultiPoint, LineString, MultiLineString 2.2 Prepare data for processing # read in roads shapefile roads = gpd.read_file(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) roads.head() # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) unique_locs = chkCovars.drop_duplicates(subset=[&#39;longitude&#39;, &#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;]] unique_locs[&#39;coordId&#39;] = np.arange(1, unique_locs.shape[0]+1) chkCovars = chkCovars.merge(unique_locs, on=[&#39;longitude&#39;, &#39;latitude&#39;]) unique_locs = gpd.GeoDataFrame( unique_locs, geometry=gpd.points_from_xy(unique_locs.longitude, unique_locs.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 roads = roads.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) # function to simplify multilinestrings def simplify_roads(complex_roads): simpleRoads = [] for i in range(len(complex_roads.geometry)): feature = complex_roads.geometry.iloc[i] if feature.geom_type == &quot;LineString&quot;: simpleRoads.append(feature) elif feature.geom_type == &quot;MultiLineString&quot;: for road_level2 in feature: simpleRoads.append(road_level2) return simpleRoads # function to use ckdtrees for nearest point finding def ckdnearest(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) simplified_features = simplify_roads(gdfB) B = [np.array(geom.coords) for geom in simplified_features] B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=1) return dist # function to use ckdtrees for nearest point finding def ckdnearest_point(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate( [np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) return dist # get distance to nearest road unique_locs[&#39;dist_road&#39;] = ckdnearest(unique_locs, roads) # get distance to nearest other site unique_locs[&#39;nnb&#39;] = ckdnearest_point(unique_locs, unique_locs) # write to file unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;dist_road&#39;] = unique_locs[&#39;dist_road&#39;] unique_locs[&#39;nnb&#39;] = unique_locs[&#39;nnb&#39;] unique_locs.to_csv(path_or_buf=&quot;data/locs_dist_to_road.csv&quot;, index=False) # merge unique locs with chkCovars chkCovars = chkCovars.merge(unique_locs, on=[&#39;latitude&#39;, &#39;longitude&#39;, &#39;coordId&#39;]) 2.3 Species specific nearest sites # load data and send to python load(&quot;data_prelim_processing.rdata&quot;) py$data &lt;- dataGrouped # split data by species datalist = [pd.DataFrame(y) for x, y in data.groupby(&#39;scientific_name&#39;, as_index=False)] # function to get unique vals anc convert to gpd def convData(somedata): somedata = somedata.drop_duplicates(subset= [&#39;longitude&#39;,&#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;, &#39;scientific_name&#39;]] unique_locs = gpd.GeoDataFrame(somedata, geometry=gpd.points_from_xy(somedata.longitude, somedata.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) dists = ckdnearest_point(unique_locs, unique_locs) unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;nnb&#39;] = dists return unique_locs # apply function to datalist datalist = list(map(convData, datalist)) 2.4 Explicit spatial filter # extract data from python chkCovars &lt;- py$chkCovars chkCovars &lt;- st_as_sf(chkCovars, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # read wg wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # spatial subset chkCovars &lt;- chkCovars %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, chkCovars))) 2.5 Species specific filter # extract values from python sp_spec_data &lt;- py$datalist sp_spec_data &lt;- map(sp_spec_data, function(df){ df &lt;- as_tibble(df) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, .))) %&gt;% st_drop_geometry() }) sp_spec_data &lt;- bind_rows(sp_spec_data) 2.6 Plot histogram: distance to roads # make histogram hist_roads &lt;- ggplot(chkCovars)+ geom_histogram(aes(dist_road / 1e3), bins = 20, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;distance to roads (km)&quot;, y = &quot;# checklists&quot;)+ scale_x_log10(label=label_number(accuracy = 0.1), breaks = c(0.1, 1, 10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) 2.7 Table: Distance to roads # write the mean and ci95 to file chkCovars %&gt;% st_drop_geometry() %&gt;% select(dist_road, nnb) %&gt;% tidyr::pivot_longer(cols = c(&quot;dist_road&quot;, &quot;nnb&quot;), names_to = &quot;variable&quot;) %&gt;% group_by(variable) %&gt;% summarise_at(vars(value), list(~mean(.), ~sd(.), ~min(.), ~max(.))) %&gt;% write_csv(&quot;data/results/distance_roads_sites.csv&quot;) # read in and show library(magrittr) readr::read_csv(&quot;data/results/distance_roads_sites.csv&quot;) %&gt;% knitr::kable() variable mean sd min max dist_road 390 859 0.279 7637 nnb 297 553 0.137 12850 2.8 Plot histogram: distance to nearest site # get unique locations locs &lt;- py$unique_locs # make histogram of nearest neighbours hist_sites &lt;- ggplot(locs)+ geom_histogram(aes(nnb / 1e3), bins = 100, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;dist. nearest site (km)&quot;, y = &quot;# sites&quot;)+ # scale_x_log10(label=label_number(accuracy = 0.1), # breaks = c(0.1, 1, 10))+ coord_cartesian(xlim=c(0,10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) 2.9 Plot species specific histograms: distance to nearest site # plot histograms by species hist_sites_sp &lt;- ggplot(sp_spec_data)+ geom_histogram(aes(nnb / 1e3), bins = 100, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;dist. nearest site (km)&quot;, y = &quot;# sites&quot;)+ # scale_x_log10(label=label_number(accuracy = 0.1), # breaks = c(0.1, 1, 10))+ facet_wrap(~scientific_name)+ scale_x_log10()+ #coord_cartesian(xlim=c(0,10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) ggsave(hist_sites_sp, filename = &quot;figs/fig_nnb_species.png&quot;) 2.10 Table: Species specific nearest site # write the mean and ci95 to file sp_spec_data %&gt;% group_by(scientific_name) %&gt;% summarise_at(vars(nnb), list(~mean(.), ~sd(.), ~ci(.), ~min(.), ~max(.))) %&gt;% write_csv(&quot;data/results/dist_nnb_species_specific.csv&quot;) # show table of distance to nearest site for each species readr::read_csv(&quot;data/results/dist_nnb_species_specific.csv&quot;) %&gt;% knitr::kable() scientific_name mean sd ci min max Alcippe poioicephala 417 808 25.0 0.137 16461 Carpodacus erythrinus 416 808 25.0 0.137 16461 Centropus sinensis 417 808 25.0 0.137 16461 Chalcophaps indica 417 808 25.0 0.137 16461 Chloropsis aurifrons 417 809 25.0 0.137 16461 Chrysocolaptes guttacristatus 417 808 25.0 0.137 16461 Cinnyris asiaticus 417 808 25.0 0.137 16461 Copsychus fulicatus 417 808 25.0 0.137 16461 Copsychus saularis 417 808 25.0 0.137 16461 Culicicapa ceylonensis 417 808 25.0 0.137 16461 Cyornis tickelliae 417 808 25.0 0.137 16461 Dicaeum erythrorhynchos 416 808 25.0 0.137 16461 Eumyias albicaudatus 417 808 25.0 0.137 16461 Hierococcyx varius 417 808 25.0 0.137 16461 Hypsipetes ganeesa 417 808 25.0 0.137 16461 Iole indica 418 810 25.1 0.137 16461 Irena puella 417 808 25.0 0.137 16461 Lanius schach 417 808 25.0 0.137 16461 Leptocoma minima 417 808 25.0 0.137 16461 Leptocoma zeylonica 417 808 25.0 0.137 16461 Motacilla maderaspatensis 417 808 25.0 0.137 16461 Myophonus horsfieldii 417 808 25.0 0.137 16461 Orthotomus sutorius 417 808 25.0 0.137 16461 Parus cinereus 417 808 25.0 0.137 16461 Passer domesticus 417 808 25.0 0.137 16461 Pellorneum ruficeps 417 808 25.0 0.137 16461 Pericrocotus cinnamomeus 417 808 25.0 0.137 16461 Pericrocotus flammeus 417 808 25.0 0.137 16461 Picus xanthopygaeus 417 808 25.0 0.137 16461 Pomatorhinus horsfieldii 417 808 25.0 0.137 16461 Psilopogon viridis 417 808 25.0 0.137 16461 Psittacula columboides 417 808 25.0 0.137 16461 Psittacula cyanocephala 417 808 25.0 0.137 16461 Pycnonotus cafer 417 808 25.0 0.137 16461 Pycnonotus jocosus 417 808 25.0 0.137 16461 Saxicola caprata 417 808 25.0 0.137 16461 Sitta frontalis 417 808 25.0 0.137 16461 Streptopelia chinensis 417 808 25.0 0.137 16461 Turdoides striata 418 810 25.1 0.137 16461 Turdus simillimus 417 808 25.0 0.137 16461 Upupa epops 417 808 25.0 0.137 16461 Zosterops palpebrosus 417 808 25.0 0.137 16461 Histograms showing the species-specific distances to nearest neighbouring site. 2.11 Plot map: points on roads roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) points &lt;- chkCovars %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% mutate(X = round_any(X, 2500), Y = round_any(Y, 2500)) points &lt;- count(points, X,Y) # add land library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) # plot on maps ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_sf(data = wg, fill= NA, col = 1)+ annotation_custom(grob = hist_roads %&gt;% ggplotGrob(), xmin = bbox[&quot;xmax&quot;] - (bbox[&quot;xmax&quot;] - bbox[&quot;xmin&quot;])/2.5, xmax = bbox[&quot;xmax&quot;], ymin = bbox[&quot;ymax&quot;] - (bbox[&quot;ymax&quot;] - bbox[&quot;ymin&quot;])/3, ymax = bbox[&quot;ymax&quot;])+ geom_tile(data=points, aes(X,Y,fill=n), col = &quot;grey90&quot;)+ geom_sf(data=roads, size=0.2, col=&quot;steelblue&quot;)+ # scale_colour_manual(values = &quot;steelblue&quot;, labels = &quot;roads&quot;)+ scale_fill_scico(trans = &quot;log10&quot;, palette = &quot;lajolla&quot;, values=c(0, 1))+ annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering) + annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = c(0.9,0.55), legend.background = element_blank(), legend.key = element_rect(fill=&quot;grey90&quot;), axis.title = element_blank(), panel.background = element_rect(fill=&quot;lightblue&quot;))+ coord_sf(expand = FALSE, xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ labs(fill = &quot;checklists&quot;, colour=NULL) # save figure ggsave(filename = &quot;figs/fig_distRoads.png&quot;, device = png()) dev.off() # transform points to utm locs &lt;- locs %&gt;% st_as_sf(coords=c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # add nnb to locations ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_sf(data = wg, fill= NA, col = 1)+ annotation_custom(grob = hist_sites %&gt;% ggplotGrob(), xmin = bbox[&quot;xmax&quot;] - (bbox[&quot;xmax&quot;] - bbox[&quot;xmin&quot;])/2.5, xmax = bbox[&quot;xmax&quot;], ymin = bbox[&quot;ymax&quot;] - (bbox[&quot;ymax&quot;] - bbox[&quot;ymin&quot;])/3, ymax = bbox[&quot;ymax&quot;])+ geom_sf(data=roads, size=0.2, col=&quot;steelblue&quot;)+ geom_sf(data=locs, aes(col=nnb/1000))+ scale_colour_scico(palette = &quot;oslo&quot;, values=c(0, 1), direction = -1, limits = c(0, 5), na.value = &quot;indianred&quot;)+ annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering) + annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = c(0.9,0.55), legend.background = element_blank(), legend.key = element_rect(fill=&quot;grey90&quot;), axis.title = element_blank(), panel.background = element_rect(fill=&quot;lightblue&quot;))+ coord_sf(expand = FALSE, xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ labs(fill = &quot;checklists&quot;, colour=NULL) Checklist locations across the Nilgiris, Anamalais and the Palani hills. Inset histogram shows checklists’ distance to the nearest road, with the X-axis on a log-scale. "],
["species-observation-distributions.html", "Section 3 Species observation distributions 3.1 Prepare libraries 3.2 Read species of interest 3.3 Load raw data for locations 3.4 Get proportional obs counts in 25km cells 3.5 Which species are reported sufficiently in checklists? 3.6 Plot maps 3.7 Write the new list of species", " Section 3 Species observation distributions 3.1 Prepare libraries # load libraries library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(readr) library(ggplot2) library(ggthemes) library(scico) # round any function round_any &lt;- function(x, accuracy = 25000){round(x/accuracy)*accuracy} 3.2 Read species of interest # add species of interest specieslist &lt;- read.csv(&quot;data/species_list.csv&quot;) speciesOfInterest &lt;- specieslist$scientific_name 3.3 Load raw data for locations # read in shapefile of the study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;); box &lt;- st_bbox(wg) # read in data and subset ebd &lt;- fread(&quot;data/eBirdDataWG_filtered.txt&quot;) ebd &lt;- ebd[between(LONGITUDE, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(LATITUDE, box[&quot;ymin&quot;], box[&quot;ymax&quot;]),] ebd &lt;- ebd[year(`OBSERVATION DATE`) &gt;= 2013,] # make new column names newNames &lt;- str_replace_all(colnames(ebd), &quot; &quot;, &quot;_&quot;) %&gt;% str_to_lower() setnames(ebd, newNames) # keep useful columns columnsOfInterest &lt;- c(&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;, &quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;, &quot;sampling_event_identifier&quot;) ebd &lt;- ebd[, ..columnsOfInterest] # strict spatial filter and assign grid locs &lt;- ebd[,.(longitude, latitude)] # transform to UTM and get 20km boxes coords &lt;- setDF(locs) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # convert wg to UTM for filter wg &lt;- st_transform(wg, 32643) coords &lt;- coords %&gt;% filter(id %in% unlist(st_contains(wg, coords))) %&gt;% rename(longitude = X, latitude = Y) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% as.data.table() # remove unneeded objs rm(locs); gc() coords &lt;- coords[,.N,by=.(longitude, latitude, X, Y)] ebd &lt;- merge(ebd, coords, all = FALSE, by = c(&quot;longitude&quot;, &quot;latitude&quot;)) ebd &lt;- ebd[(longitude %in% coords$longitude) &amp; (latitude %in% coords$latitude),] 3.4 Get proportional obs counts in 25km cells # round to 25km cell in UTM coords ebd[, `:=`(X = round_any(X), Y = round_any(Y))] # count checklists in cell ebd_summary &lt;- ebd[,nchk := length(unique(sampling_event_identifier)), by=.(X,Y)] # count checklists reporting each species in cell and get proportion ebd_summary &lt;- ebd_summary[,.(nrep = length(unique(sampling_event_identifier))), by = .(X,Y,nchk,scientific_name)] ebd_summary[,p_rep := nrep/nchk ] # filter for soi ebd_summary &lt;- ebd_summary[scientific_name %in% speciesOfInterest,] # complete the dataframe for no reports # keep no reports as NA --- allows filtering based on proportion reporting ebd_summary &lt;- setDF(ebd_summary) %&gt;% complete(nesting(X,Y), scientific_name #, # fill = list(p_rep = 0) ) %&gt;% filter(!is.na(p_rep)) 3.5 Which species are reported sufficiently in checklists? # A total of 42 unique grids (of 25km by 25km) across the study area # total number of checklists across unique grids tot_n_chklist &lt;- ebd_summary %&gt;% distinct(X,Y,nchk) # species-specific number of grids spp_grids &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% distinct(X,Y) %&gt;% count(scientific_name, name = &quot;n_grids&quot;) # Write the above two results write_csv(tot_n_chklist,&quot;data/nchk_per_grid.csv&quot;) write_csv(spp_grids,&quot;data/ngrids_per_spp.csv&quot;) # left-join the datasets ebd_summary &lt;- left_join(ebd_summary, spp_grids, by=&quot;scientific_name&quot;) # check the proportion of grids across which this cut-off is met for each species # Is it &gt; 90% or 70%? # For example, with a 3% cut-off, ~100 species are occurring in &gt;50% of the grids they have been reported in p_cutoff &lt;- 0.05 # Proportion of checklists a species has been reported in grid_proportions &lt;- ebd_summary %&gt;% group_by(scientific_name) %&gt;% tally(p_rep&gt;=p_cutoff) %&gt;% mutate(prop_grids_cut = n/(spp_grids$n_grids))%&gt;% arrange(desc(prop_grids_cut)) grid_prop_cut &lt;- filter(grid_proportions, prop_grids_cut &gt; p_cutoff) # Write the results write_csv(grid_prop_cut, &quot;data/chk_3_percent.csv&quot;) # Identifying the number of species that occur in potentially &lt;5% of all lists total_number_lists &lt;- sum(tot_n_chklist$nchk) spp_sum_chk &lt;- ebd_summary %&gt;% distinct(X,Y, scientific_name, nrep) %&gt;% group_by(scientific_name) %&gt;% mutate(sum_chk = sum(nrep)) %&gt;% distinct(scientific_name,sum_chk) # Approximately 90 to 100 species occur in &gt;5% of all checklists prop_all_lists &lt;- spp_sum_chk %&gt;% mutate(prop_lists = sum_chk/total_number_lists) %&gt;% arrange(desc(prop_lists)) 3.6 Plot maps # add land library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) # crop land land &lt;- st_transform(land, 32643) # make plot wg &lt;- st_transform(wg, 32643) bbox &lt;- st_bbox(wg) #get a plot of number of checklists across grids plotNchk &lt;- ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_tile(data = tot_n_chklist, aes(X, Y, fill = nchk), lwd = 0.5, col = &quot;grey90&quot;)+ geom_sf(data = wg, fill = NA, col = &quot;black&quot;, lwd = 0.3)+ scale_fill_scico(palette = &quot;lajolla&quot;, direction = 1, trans = &quot;log10&quot;, limits = c(1, 10000), breaks = 10 ^ c(1:4))+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ theme_few()+ theme(legend.position = &quot;right&quot;, axis.title = element_blank(), axis.text.y = element_text(angle = 90), panel.background = element_rect(fill = &quot;lightblue&quot;))+ labs(fill = &quot;number\\nof\\nchecklists&quot;) # export data ggsave(plotNchk, filename = &quot;figs/fig_number_checklists_10km.png&quot;,height = 12, width = 7, device = png(), dpi = 300); dev.off() # filter list of species ebd_filter &lt;- semi_join(ebd_summary, grid_prop_cut, by=&quot;scientific_name&quot;) plotDistributions &lt;- ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_tile(data = ebd_filter, aes(X, Y, fill = p_rep), lwd = 0.5, col = &quot;grey90&quot;)+ geom_sf(data = wg, fill = NA, col = &quot;black&quot;, lwd = 0.3)+ scale_fill_scico(palette = &quot;lajolla&quot;, direction = 1, label = scales::percent)+ facet_wrap(~scientific_name, ncol = 12)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ ggthemes::theme_few(base_family = &quot;TT Arial&quot;, base_size = 8)+ theme(legend.position = &quot;right&quot;, strip.text = element_text(face = &quot;italic&quot;), axis.title = element_blank(), axis.text.y = element_text(angle = 90), panel.background = element_rect(fill = &quot;lightblue&quot;))+ labs(fill = &quot;prop.\\nreporting\\nchecklists&quot;) # export data ggsave(plotDistributions, filename = &quot;figs/fig_species_distributions.png&quot;, height = 25,width = 25, device = png(), dpi = 300); dev.off() Proportion of checklists reporting a species in each grid cell (20km side) between 2013 and 2018. Checklists were filtered to be within the boundaries of the Nilgiris, Anamalais and the Palani hills (black outline), but rounding to 20km cells may place cells outside the boundary. Deeper shades of red indicate a higher proportion of checklists reporting a species. 3.7 Write the new list of species # write the new list of species that occur in at least 3% of checklists across a minimum of 50% of the grids they have been reported in new_sp_list &lt;- semi_join(specieslist,grid_prop_cut, by=&quot;scientific_name&quot;) write_csv(new_sp_list,&quot;data/3_List_of_species_cutoff.csv&quot;, row.names = F) "],
["climate-in-relation-to-elevation.html", "Section 4 Climate in relation to elevation 4.1 Prepare libraries 4.2 Plot climatic variables over elevation", " Section 4 Climate in relation to elevation 4.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) library(scales) library(ggplot2) library(ggthemes) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;, &quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;elev&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, getValues) names(land_data) &lt;- c(&quot;elev&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% mutate(elev_round = plyr::round_any(elev, 200)) %&gt;% select(-elev) %&gt;% pivot_longer(cols = contains(&quot;chelsa&quot;), names_to = &quot;clim_var&quot;) %&gt;% group_by(elev_round, clim_var) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) 4.2 Plot climatic variables over elevation # plot in facets fig_climate_elev &lt;- ggplot(land_data)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m)&quot;, y = &quot;CHELSA variable value&quot;) # save as png ggsave(fig_climate_elev, filename = &quot;figs/fig_climate_elev.png&quot;, height = 4, width = 6, device = png(), dpi = 300); dev.off() CHELSA climatic variables as a function of elevation, in increments of 200m. Points represent means, while vertical lines show 95% confidence intervals. "],
["landcover-in-relation-to-elevation.html", "Section 5 Landcover in relation to elevation 5.1 Get data from landscape rasters 5.2 Plot proportional landcover in elevation", " Section 5 Landcover in relation to elevation 5.1 Get data from landscape rasters # get data from landscape rasters lc_elev &lt;- tibble(elev = getValues(landscape[[&quot;elev&quot;]]), landcover = getValues(landscape[[&quot;landcover&quot;]])) # process data for proportions lc_elev &lt;- lc_elev %&gt;% filter(!is.na(landcover), landcover != 0) %&gt;% mutate(elev = plyr::round_any(elev, 100)) %&gt;% count(elev, landcover) %&gt;% group_by(elev) %&gt;% mutate(prop = n/sum(n)) 5.2 Plot proportional landcover in elevation # plot figure as tilemap fig_lc_elev &lt;- ggplot(lc_elev)+ geom_tile(aes(x=elev, y=factor(landcover), fill=prop), col=&quot;grey99&quot;, size = 0.6)+ scale_fill_scico(palette = &quot;bilbao&quot;, begin = 0.0, end = 1.0)+ scale_x_continuous(breaks = seq(0, 2500, 500), labels = comma)+ scale_alpha_continuous(range = c(0.3, 1))+ labs(x = &quot;elevation (m)&quot;, y = &quot;landcover&quot;)+ theme_few() # export figure ggsave(fig_lc_elev, filename = &quot;figs/fig_lc_elev.png&quot;, height = 3, width = 6, device = png(), dpi = 300); dev.off() Proportional landcover (low = white, high = dark red), as a function of elevation in the study site. Data represent elevation in increments of 100m. "],
["climate-in-relation-to-landcover.html", "Section 6 Climate in relation to landcover 6.1 Prepare libraries 6.2 Plot climatic variables over landcover", " Section 6 Climate in relation to landcover 6.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) # plotting options library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;, &quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;landcover&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, raster::getValues) names(land_data) &lt;- c(&quot;landcover&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% filter(landcover != 0) %&gt;% pivot_longer(cols = contains(&quot;chelsa&quot;), names_to = &quot;clim_var&quot;) #%&gt;% # group_by(landcover, clim_var) %&gt;% # summarise_all(.funs = list(~mean(.), ~ci(.))) 6.2 Plot climatic variables over landcover # plot in facets fig_climate_lc &lt;- ggplot(land_data)+ geom_jitter(aes(x = landcover-0.25, y=value, col = factor(landcover)), width = 0.2, size = 0.1, alpha = 0.1, shape = 4)+ geom_boxplot(aes(x = landcover+0.25, y = value, group = landcover), width = 0.2, outlier.size = 0.2, alpha = 0.3, fill = NA)+ scale_colour_scico_d(begin=0.2, end=0.8)+ scale_y_continuous(labels = scales::comma)+ scale_x_continuous(breaks = c(1:7))+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;landcover class&quot;, y = &quot;CHELSA variable value&quot;) # save as png ggsave(fig_climate_lc, filename = &quot;figs/fig_climate_landcover.png&quot;, height = 5, width = 8, device = png(), dpi = 300); dev.off() CHELSA climatic variables as a function of landcover class. Grey points in the background represent raw data. "],
["obsever-expertise-in-time-and-space.html", "Section 7 Obsever expertise in time and space 7.1 Prepare libraries 7.2 Species seen in relation to osberver expertise 7.3 Expertise in relation to landcover", " Section 7 Obsever expertise in time and space 7.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) library(readr) library(scales) # plotting libs library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read in scores and checklist data and link scores &lt;- read_csv(&quot;data/dataObsExpScore.csv&quot;) data &lt;- read_csv(&quot;data/eBirdChecklistVars.csv&quot;) data &lt;- left_join(data, scores, by = c(&quot;observer&quot; = &quot;observer&quot;)) data &lt;- select(data, score, nSp, nSoi, landcover, year) %&gt;% filter(!is.na(score)) 7.2 Species seen in relation to osberver expertise # summarise data by rounded score and year data_summary01 &lt;- data %&gt;% mutate(score = plyr::round_any(score, 0.2)) %&gt;% select(score, year, nSp, nSoi) %&gt;% pivot_longer(cols = c(&quot;nSp&quot;, &quot;nSoi&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% group_by(score, year, variable) %&gt;% summarise_at(vars(value), list(~mean(.), ~ci(.))) # make plot and export fig_nsp_score &lt;- ggplot(data_summary01)+ geom_jitter(data = data, aes(x = score, y = nSp), col = &quot;grey&quot;, alpha = 0.2, size = 0.1)+ geom_pointrange(aes(x = score, y = mean, ymin=mean-ci, ymax=mean+ci, col = as.factor(variable)), position = position_dodge(width = 0.05))+ facet_wrap(~year)+ scale_y_log10()+ # coord_cartesian(ylim=c(0,50))+ scale_colour_scico_d(palette = &quot;cork&quot;, begin = 0.2, end = 0.8)+ labs(x = &quot;expertise score&quot;, y = &quot;species reported&quot;)+ theme_few()+ theme(legend.position = &quot;none&quot;) # export figure ggsave(filename = &quot;figs/fig_nsp_score.png&quot;, width = 8, height = 6, device = png(), dpi = 300); dev.off() Total number of species (blue) and species of interest to this study (green) reported in checklists from the study area over the years 2013 – 2018, as a function of the expertise score of the reporting observer. Points represent means, with bars showing the 95% confidence intervals; data shown are for expertise scores rounded to multiples of 0.2, and the y-axis is on a log scale. Raw data are shown in the background (grey points). 7.3 Expertise in relation to landcover # plot histograms of expertise scores in different landcover classes data &lt;- filter(data, !is.na(landcover)) # make plot fig_exp_lc &lt;- ggplot(data)+ geom_histogram(aes(x = score), fill = &quot;steelblue&quot;, bins = 20)+ facet_wrap(~landcover, scales = &quot;free_y&quot;, labeller = label_both, nrow = 2)+ scale_y_continuous(labels = comma)+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;expertise score&quot;, y = &quot;count&quot;) # export figure ggsave(filename = &quot;figs/fig_exp_lc.png&quot;, width = 8, height = 4, device = png(), dpi = 300); dev.off() Distribution of expertise scores in the seven landcover classes present in the study site. "],
["spatial-autocorrelation-in-climatic-predictors.html", "Section 8 Spatial autocorrelation in climatic predictors 8.1 Load libs and prep data 8.2 Calculate variograms 8.3 Plot CHELSA data and variograms", " Section 8 Spatial autocorrelation in climatic predictors 8.1 Load libs and prep data # load libs library(raster) library(gstat) library(stars) library(purrr) library(tibble) library(dplyr) library(tidyr) library(glue) library(scales) library(gdalUtils) # plot libs library(ggplot2) library(ggthemes) library(scico) library(gridExtra) library(cowplot) library(ggspatial) #&#39;make custom functiont to convert matrix to df raster_to_df &lt;- function(inp) { # assert is a raster obj assertthat::assert_that(&quot;RasterLayer&quot; %in% class(inp), msg = &quot;input is not a raster&quot;) coords &lt;- coordinates(inp) vals &lt;- getValues(inp) data &lt;- tibble(x = coords[,1], y = coords[,2], value = vals) return(data) } # list landscape covariate stacks landscape_files &lt;- &quot;data/spatial/landscape_resamp01km.tif&quot; landscape_data &lt;- stack(landscape_files) # get proper names { elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;,&quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) } # get chelsa rasters chelsa &lt;- landscape_data[[chelsa_names]] chelsa &lt;- purrr::map(as.list(chelsa), raster_to_df) 8.2 Calculate variograms # prep variograms vgrams &lt;- purrr::map(chelsa, function(z){ z &lt;- drop_na(z) vgram &lt;- gstat::variogram(value~1, loc=~x+y, data = z) return(vgram) }) # save temp save(vgrams, file = &quot;data/chelsa/chelsaVariograms.rdata&quot;) # get variogram data vgrams &lt;- purrr::map(vgrams, function(df){ df %&gt;% select(dist, gamma) }) vgrams &lt;- tibble(variable = chelsa_names, data = vgrams) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) # add lamd library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) # crop land land &lt;- st_transform(land, 32643) 8.3 Plot CHELSA data and variograms # make ggplot of variograms yaxis &lt;- c(&quot;semivariance&quot;, rep(&quot;&quot;, 4)) xaxis &lt;- c(&quot;&quot;, &quot;&quot;, &quot;distance (km)&quot;, &quot;&quot;, &quot;&quot;) fig_vgrams &lt;- purrr::pmap(list(vgrams$data, yaxis, xaxis), function(df, ya, xa){ ggplot(df)+ geom_line(aes(x = dist/1000, y = gamma), size = 0.2, col = &quot;grey&quot;)+ geom_point(aes(x = dist/1000, y = gamma), col = &quot;black&quot;)+ scale_x_continuous(labels = comma, breaks = c(seq(0,100,25)))+ scale_y_log10(labels = comma)+ labs(x = xa, y = ya)+ theme_few()+ theme(axis.text.y = element_text(angle = 90, hjust = 0.5, size = 8), strip.text = element_blank()) }) fig_vgrams &lt;- purrr::map(fig_vgrams, as_grob) # make ggplot of chelsa data chelsa &lt;- as.list(landscape_data[[chelsa_names]]) %&gt;% purrr::map(st_as_stars) # colour palettes pal &lt;- c(&quot;bilbao&quot;, &quot;davos&quot;, &quot;davos&quot;, &quot;nuuk&quot;, &quot;bilbao&quot;) title &lt;- c(&quot;a Temp. seasonality&quot;, &quot;b Ppt. driest qtr.&quot;, &quot;c Ppt. warmest qtr.&quot;, &quot;d Variation ppt.&quot;, &quot;e Variation temp.&quot;) direction &lt;- c(1,-1,-1,-1,1) lims &lt;- list(range(chelsa[[1]]$chelsa_bio10_04, na.rm = T), c(0, 500), c(0, 500), c(0,500),#range(chelsa[[4]]$chelsa_prec, na.rm = T), range(chelsa[[5]]$chelsa_temp, na.rm = T)) fig_list_chelsa &lt;- purrr::pmap(list(chelsa, pal, title, direction, lims), function(df, pal, t, d, l){ ggplot()+ geom_stars(data = df)+ geom_sf(data = land, fill = NA, colour = &quot;black&quot;)+ geom_sf(data = wg, fill = NA, colour = &quot;black&quot;, size = 0.3)+ scale_fill_scico(palette = pal, direction = d, label = comma, na.value = NA, limits = l)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ annotation_scale(location = &quot;tr&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = &quot;top&quot;, title = element_text(face = &quot;bold&quot;, size = 8), legend.key.height = unit(0.2, &quot;cm&quot;), legend.key.width = unit(1, &quot;cm&quot;), legend.text = element_text(size = 8), axis.title = element_blank(), axis.text.y = element_text(angle = 90, hjust = 0.5), # panel.background = element_rect(fill = &quot;lightblue&quot;), legend.title = element_blank())+ labs(x=NULL, y=NULL, title = t) }) fig_list_chelsa &lt;- purrr::map(fig_list_chelsa, as_grob) fig_list_chelsa &lt;- append(fig_list_chelsa, fig_vgrams) lmatrix &lt;- matrix(c(c(1,2,3,4,5), c(1,2,3,4,5), c(6,7,8,9,10)), nrow = 3, byrow = T) plot_grid &lt;- grid.arrange(grobs = fig_list_chelsa, layout_matrix = lmatrix) ggsave(plot = plot_grid, filename = &quot;figs/fig_chelsa_variograms.png&quot;, dpi = 300, width = 12, height = 6) ``` "],
["climatic-raster-resampling.html", "Section 9 Climatic raster resampling 9.1 Prepare landcover 9.2 Prepare spatial extent 9.3 Prepare CHELSA rasters 9.4 Resample prepared rasters", " Section 9 Climatic raster resampling 9.1 Prepare landcover # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/Reprojected Image_26thJan2020_UTM_Ghats.tif&quot; # get extent e = bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- map(c(100, 250, 500, 1e3, 2.5e3), function(x){x*res_init}) # use gdalutils gdalwarp for resampling transform # to 1km from 10m for (i in 1:length(res_final)) { this_res &lt;- res_final[[i]] this_res_char &lt;- stringr::str_pad(this_res[1], 5, pad = &quot;0&quot;) gdalwarp(srcfile = landcover, dstfile = as.character(glue(&#39;data/landUseClassification/lc_{this_res_char}m.tif&#39;)), tr=c(this_res), r=&#39;mode&#39;, te=c(e)) } # read in resampled landcover raster files as a list lc_files &lt;- list.files(&quot;data/landUseClassification/&quot;, pattern = &quot;lc&quot;, full.names = TRUE) lc_data &lt;- map(lc_files, raster) 9.2 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) bbox &lt;- st_bbox(hills) 9.3 Prepare CHELSA rasters # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(buffer) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) names(chelsaData) &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;,&quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) 9.4 Resample prepared rasters # make resampled data resamp_data &lt;- map(lc_data, function(this_scale){ rr &lt;- projectRaster(from = chelsaData, to = this_scale, crs = crs(this_scale), res = res(this_scale)) }) # make a stars list resamp_data &lt;- map2(resamp_data, lc_data, function(z1,z2){ z2[z2 == 0] &lt;- NA z2 &lt;- append(z2, as.list(z1)) %&gt;% map(st_as_stars) }) %&gt;% flatten() # colour palettes pal &lt;- c(&quot;batlow&quot;, &quot;bilbao&quot;, &quot;davos&quot;, &quot;davos&quot;, &quot;nuuk&quot;, &quot;bilbao&quot;) title &lt;- c(&quot;a landcover&quot;, &quot;b Temp. seasonality&quot;, &quot;c Ppt. driest qtr.&quot;, &quot;d Ppt. warmest qtr.&quot;, &quot;e Variation ppt.&quot;, &quot;f Variation temp.&quot;) title &lt;- c(title, rep(&quot;&quot;, 24)) direction &lt;- c(1,1,-1,-1,-1,1) scales &lt;- c(c(&quot;1.0km&quot;, rep(&quot;&quot;, 5)), c(&quot;2.5km&quot;, rep(&quot;&quot;, 5)), c(&quot;5.0km&quot;, rep(&quot;&quot;, 5)), c(&quot;10km&quot;, rep(&quot;&quot;, 5)), c(&quot;25km&quot;, rep(&quot;&quot;, 5))) # make figures across the list fig_list_chelsa_resamp &lt;- purrr::pmap(list(resamp_data, scales, rep(pal, 5), title, rep(direction, 5)), function(df, scale, pal, t, d){ ggplot()+ geom_stars(data = df)+ geom_sf(data = hills, fill = NA, colour = &quot;black&quot;, size = 0.3)+ scale_fill_scico(palette = pal, direction = d, label = comma, na.value = NA)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ theme_void()+ theme(#legend.position = &quot;top&quot;, panel.border = element_rect(), title = element_text(face = &quot;bold&quot;, size = 8), # legend.key.height = unit(0.1, &quot;cm&quot;), # legend.key.width = unit(0.6, &quot;cm&quot;), # legend.text = element_text(size = 8), axis.title = element_text(), axis.title.y = element_text(angle = 90), # axis.text.y = element_text(angle = 90, hjust = 0.5), # panel.background = element_rect(fill = &quot;lightblue&quot;), legend.title = element_blank())+ labs(x=NULL, y=scale, title = t) }) fig_list_chelsa_resamp &lt;- purrr::map(fig_list_chelsa_resamp, as_grob) fig_chelsa_resamp &lt;- grid.arrange(grobs = fig_list_chelsa_resamp, ncol=6) ggsave(plot = fig_chelsa_resamp, filename = &quot;figs/fig_chelsa_resamp.png&quot;, dpi = 100, width = 24, height = 12, device = png(), units = &quot;in&quot;) # use magick to convert library(magick) pl &lt;- image_read_pdf(&quot;figs/fig_chelsa_resamp.pdf&quot;) image_write(pl, path = &quot;figs/fig_chelsa_resamp.png&quot;, format = &quot;png&quot;) CHELSA rasters with study area outline, at different scales. Semivariograms are on a log-transformed y-axis. "],
["matching-effort-with-spatial-independence.html", "Section 10 Matching effort with spatial independence 10.1 Load librarires 10.2 Load data 10.3 Plot distance exclusion effect", " Section 10 Matching effort with spatial independence How many sites would be lost if effort distance was restricted based on spatial independence? 10.1 Load librarires # load data packagaes library(data.table) library(dplyr) # load plotting packages library(ggplot2) library(scico) library(ggthemes) library(scales) 10.2 Load data # load checklist covariates data &lt;- fread(&quot;data/eBirdChecklistVars.csv&quot;) effort_distance_summary &lt;- data[, effort_distance_class := cut(distance, breaks = c(-1, 0.001, 0.1, 0.25, 0.5, 1, 2.5, 5, Inf), ordered_result = T) ][,.N, by = effort_distance_class ][order(effort_distance_class)] effort_distance_summary[,prop_effort:=cumsum(effort_distance_summary$N)/nrow(data)] 10.3 Plot distance exclusion effect # plot effort distance class cumulative sum fig_dist_exclusion &lt;- ggplot(effort_distance_summary)+ geom_point(aes(effort_distance_class, prop_effort), size = 3)+ geom_path(aes(effort_distance_class, prop_effort, group = NA))+ # scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ scale_x_discrete(labels = c(&quot;stationary&quot;, &quot;100m&quot;, &quot;250m&quot;, &quot;500m&quot;, &quot;1 km&quot;, &quot;2.5 km&quot;, &quot;5 km&quot;))+ theme_few()+ theme(panel.grid = element_line(size = 0.2, color = &quot;grey&quot;))+ labs(x = &quot;effort distance cutoff&quot;, y = &quot;proportion of checklists&quot;) ggsave(plot = fig_dist_exclusion, &quot;figs/fig_cutoff_effort.png&quot;, height = 6, width = 8, dpi = 300) dev.off() "],
["spatial-thinning-a-comparison-of-approaches.html", "Section 11 Spatial thinning: A comparison of approaches 11.1 Prepare libraries 11.2 Traditional grid-based thinning 11.3 Network-based thinning 11.4 Finding modularity in proximity networks 11.5 Process proximity networks in R 11.6 A function that removes sites 11.7 Removing non-independent sites 11.8 Measuring method fallibility 11.9 Prepare data for Python 11.10 Count props under threshold in Python 11.11 Plot metrics for different methods", " Section 11 Spatial thinning: A comparison of approaches 11.1 Prepare libraries # load libraries library(tidyverse) library(glue) library(readr) library(sf) # plotting library(ggthemes) library(scico) library(scales) # ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # load python libs here library(reticulate) # set python path use_python(&quot;/usr/bin/python3&quot;) 11.2 Traditional grid-based thinning # load the shapefile of the study area wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # get scales # load checklist data and select one per rounded 500m coordinates { data &lt;- read_csv(&quot;data/eBirdChecklistVars.csv&quot;) %&gt;% count(longitude, latitude, name = &quot;tot_effort&quot;) # how many unique points n_all_points &lt;- nrow(data) d_all_effort &lt;- sum(data$tot_effort) # round to different scales scale &lt;- c(100, 250, 500, 1000) # group data by scale data &lt;- crossing(scale, data) %&gt;% group_by(scale) %&gt;% nest() %&gt;% ungroup() } # select one point per grid cell data &lt;- mutate(data, data = map2(scale, data, function(sc, df){ # transform the data df &lt;- df %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% mutate(coordId = 1:nrow(.), X_round = plyr::round_any(X, sc), Y_round = plyr::round_any(Y, sc)) # make a grid grid &lt;- st_make_grid(wg, cellsize = sc) # which cell contains which points grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) rm(grid) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) # get summary for max max_sites &lt;- points_max %&gt;% ungroup() %&gt;% summarise(prop_points= length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;) # select a random point in each grid points_rand &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% sample_n(size = 1) # get summary for rand rand_sites &lt;- points_rand %&gt;% ungroup() %&gt;% summarise(prop_points = length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;) df &lt;- tibble(grid_rand = list(rand_sites), grid_max = list(max_sites), points_rand = list(points_rand), points_max = list(points_max)) })) # unnest data data &lt;- unnest(data, cols = data) # save summary as another object data_thin_trad &lt;- data %&gt;% select(-contains(&quot;points&quot;)) %&gt;% pivot_longer(cols = -contains(&quot;scale&quot;), names_to = &quot;method&quot;, values_to = &quot;somedata&quot;) %&gt;% unnest(cols = somedata) # save points for later comparison points_thin_trad &lt;- data %&gt;% select(contains(&quot;points&quot;), scale) rm(data) 11.3 Network-based thinning Load python libraries. # import classic python libs import numpy as np import matplotlib.pyplot as plt # libs for dataframes import pandas as pd # network lib import networkx as nx # import libs for geodata import geopandas as gpd # import ckdtree from scipy.spatial import cKDTree 11.4 Finding modularity in proximity networks # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) ul = chkCovars[[&#39;longitude&#39;, &#39;latitude&#39;]].drop_duplicates(subset=[&#39;longitude&#39;, &#39;latitude&#39;]) ul[&#39;coordId&#39;] = np.arange(0, ul.shape[0]) # get effort at each coordinate effort = chkCovars.groupby([&#39;longitude&#39;, &#39;latitude&#39;]).size().to_frame(&#39;tot_effort&#39;) effort = effort.reset_index() # merge effort on ul ul = pd.merge(ul, effort, on=[&#39;longitude&#39;, &#39;latitude&#39;]) # make gpd and drop col from ul ulgpd = gpd.GeoDataFrame(ul, geometry=gpd.points_from_xy(ul.longitude, ul.latitude)) ulgpd.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 ulgpd = ulgpd.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) ul = pd.DataFrame(ul.drop(columns=&quot;geometry&quot;)) # function to use ckdtrees for nearest point finding def ckd_pairs(gdfA, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) ckd_tree = cKDTree(A) dist = ckd_tree.query_pairs(r=dist_indep, output_type=&#39;ndarray&#39;) return dist # define scales in metres scales = [100, 250, 500, 1000] # function to process ckd_pairs def make_modules(scale): site_pairs = ckd_pairs(gdfA=ulgpd, dist_indep=scale) site_pairs = pd.DataFrame(data=site_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) site_pairs[&#39;scale&#39;] = scale # get site ids site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) site_id = np.unique(site_id) # make network network = nx.from_pandas_edgelist(site_pairs, &#39;p1&#39;, &#39;p2&#39;) # get modules modules = list(nx.algorithms.community.greedy_modularity_communities(network)) # get modules as df m = [] for i in np.arange(len(modules)): module_number = [i] * len(modules[i]) module_coords = list(modules[i]) m = m + list(zip(module_number, module_coords)) # add location and summed sampling duration unique_locs = ul[ul.coordId.isin(site_id)] module_data = pd.DataFrame(m, columns=[&#39;module&#39;, &#39;coordId&#39;]) module_data = pd.merge(module_data, unique_locs, on=&#39;coordId&#39;) # add scale module_data[&#39;scale&#39;] = scale return [site_pairs, module_data] # run make modules on ulgpd at scales data = list(map(make_modules, scales)) # extract data for output tot_pair_data = [] tot_module_data = [] for i in np.arange(len(data)): tot_pair_data.append(data[i][0]) tot_module_data.append(data[i][1]) tot_pair_data = pd.concat(tot_pair_data, ignore_index=True) tot_module_data = pd.concat(tot_module_data, ignore_index=True) # make dict of positions and array of coordinates # site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) # site_id = np.unique(site_id) # locations_df = ul[ul.coordId.isin(site_id)][[&#39;longitude&#39;, &#39;latitude&#39;]].to_numpy() # pos_dict = dict(zip(site_id, locations_df)) # output data tot_module_data.to_csv(path_or_buf=&quot;data/site_modules.csv&quot;, index=False) tot_pair_data.to_csv(path_or_buf=&quot;data/site_pairs.csv&quot;, index=False) # ends here 11.5 Process proximity networks in R # read in pair and module data pairs &lt;- read_csv(&quot;data/site_pairs.csv&quot;) mods &lt;- read_csv(&quot;data/site_modules.csv&quot;) # count pairs at each scale count(pairs, scale) pairs %&gt;% group_by(scale) %&gt;% summarise(non_indep_pairs = length(unique(c(p1, p2)))/n_all_points) count(mods, scale) # nest by scale and add module data data &lt;- nest(pairs, data = c(p1, p2)) modules &lt;- group_by(mods, scale) %&gt;% nest() %&gt;% ungroup() # add module data data &lt;- mutate(data, modules = modules$data, data = map2(data, modules, function(df, m){ df &lt;- left_join(df, m, by = c(&quot;p1&quot; = &quot;coordId&quot;)) df &lt;- left_join(df, m, by = c(&quot;p2&quot; = &quot;coordId&quot;)) df &lt;- filter(df, module.x == module.y) return(df) })) %&gt;% select(-modules) # split by module data$data &lt;- map(data$data, function(df){ df &lt;- group_by(df, module.x, module.y) %&gt;% nest() %&gt;% ungroup() return(df) }) 11.6 A function that removes sites # a function to remove sites remove_which_sites &lt;- function(pair_data){ { a = pair_data %&gt;% select(p1, p2) nodes_a_init = unique(c(a$p1, a$p2)) i_n_d = filter(mods, coordId %in% nodes_a_init) %&gt;% select(node = coordId, tot_effort) %&gt;% mutate(s_f_r = NA) nodes_keep = c() nodes_removed = c() } while(nrow(a) &gt; 0){ # how many nodes in a nodes_a = unique(c(a$p1, a$p2)) # get node or site efforts and arrange in ascending order b = i_n_d %&gt;% filter(node %in% nodes_a) for (i in 1:nrow(b)){ # which node to remove node_out = b$node[i] # how much tot_effort lost d_n_o = b$tot_effort[i] # how many rows remain in a if node_out is removed? a_n_o = filter(a, p1 != node_out, p2 != node_out) indep_nodes = setdiff(nodes_a, unique(c(a_n_o$p1, a_n_o$p2, node_out))) # how much sampling effort made spatially independent indep_sampling = filter(b, node %in% indep_nodes) %&gt;% summarise(tot_effort = sum(tot_effort)) %&gt;% .$tot_effort # message(glue::glue(&#39;{node_out} removal frees {indep_sampling} m&#39;)) # sampling freed by sampling lost b$s_f_r[i] = indep_sampling/d_n_o } # arrange node data by decreasing sfr and increasing tot_effort # highest tot_effort nodes are processed last b = arrange(b, -s_f_r, tot_effort) nodes_removed = c(nodes_removed, b$node[1]) # remove pairs of nodes containing the highest sfr node in b a = filter(a, p1 != b$node[1], p2 != b$node[1]) nodes_keep = c(nodes_keep, setdiff(nodes_a, unique(c(a$p1, a$p2, nodes_removed)))) } message(glue::glue(&#39;keeping {length(nodes_keep)} of {length(nodes_a_init)}&#39;)) # node_status &lt;- tibble(nodes = c(nodes_keep, nodes_removed), # status = c(rep(TRUE, length(nodes_keep)), # rep(FALSE, length(nodes_removed)))) return(as.integer(nodes_removed)) } 11.7 Removing non-independent sites # remove 5km and 2.5km scale data &lt;- data %&gt;% filter(scale &lt;=1000) # run select sites on the various modules sites_removed &lt;- map(data$data, function(df){ remove_sites &lt;- unlist(purrr::map(df$data, remove_which_sites)) }) # save as rdata save(sites_removed, file = &quot;data/data_network_sites_removed.rdata&quot;) # get python sites ul = py$ul load(&quot;data/data_network_sites_removed.rdata&quot;) # subset sites data &lt;- mutate(data, data = map(sites_removed, function(site_id){ as_tibble(filter(ul, !coordId %in% site_id)) })) # which points are kept points_thin_net &lt;- mutate(data, data = map(data,function(df){ df &lt;- df %&gt;% select(&quot;longitude&quot;, &quot;latitude&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() })) # get metrics for method data_thin_net &lt;- unnest(data, cols = &quot;data&quot;) %&gt;% group_by(scale) %&gt;% summarise(prop_points = length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% mutate(method = &quot;network&quot;) %&gt;% pivot_longer(cols = -one_of(c(&quot;method&quot;, &quot;scale&quot;)), names_to = &quot;variable&quot;) 11.8 Measuring method fallibility How many points, at different spatial scales, remain after the application of each method? 11.9 Prepare data for Python # get points by each method points_list &lt;- append(points_thin_net$data, values = append(points_thin_trad$points_rand, points_thin_trad$points_max)) # get scales as list scales_list &lt;- list(100,250,500,1000, rep(c(100,250, 500,1000), 2)) %&gt;% flatten() # send to python py$points_list = points_list py$scales_list = scales_list 11.10 Count props under threshold in Python # transform each element to gpd # a function to convert to gpd def make_gpd(df): df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y)) df.crs = {&#39;init&#39; :&#39;epsg:32643&#39;} return df # function for mean nnd # function to use ckdtrees for nearest point finding def ckd_test(gdfA, gdfB, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate([np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) dist_diff = list(map(lambda x: x - dist_indep, dist)) mean_dist_diff = np.asarray(dist_diff).mean() return mean_dist_diff # apply to all data points_list = list(map(make_gpd, points_list)) # get nnb all data mean_dist_diff = list(map(ckd_test, points_list, points_list, scales_list)) # count points above threshold # n_non_indep = [] # for i in np.arange(len(points_list)): # ni_pairs = ckd_test(gdfA=points_list[i],gdfB=points_list[i], dist_indep=scales_list[i]) # ni_pairs = pd.DataFrame(data=ni_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) # site_id = np.concatenate((ni_pairs.p1.unique(), ni_pairs.p2.unique())) # ni_sites = len(np.unique(site_id))/points_list[i].shape[0] # n_non_indep.append(ni_sites) 11.11 Plot metrics for different methods # combine the thinning metrics data data_plot &lt;- bind_rows(data_thin_net, data_thin_trad) # get data for mean distance data_thin_compare &lt;- tibble(scale = unlist(scales_list), method = c(rep(&quot;network&quot;, 4), rep(&quot;grid_rand&quot;, 4), rep(&quot;grid_max&quot;, 4)), `mean NND - buffer (m)` = unlist(py$mean_dist_diff)) %&gt;% pivot_longer(cols = &quot;mean NND - buffer (m)&quot;, names_to = &quot;variable&quot;) # bind rows with other data data_plot &lt;- bind_rows(data_plot, data_thin_compare) # plot results fig_spatial_thinning &lt;- ggplot(data_plot)+ geom_vline(xintercept = scale, lty = 3, colour=&quot;grey&quot;, lwd=0.4)+ geom_line(aes(x = scale, y= value, col =method))+ geom_point(aes(x = scale, y= value, col =method, shape = method))+ facet_wrap(~variable, scales = &quot;free&quot;)+ scale_shape_manual(values = c(1, 2, 0))+ scale_x_continuous(breaks = scale)+ scale_y_continuous()+ scale_colour_scico_d(palette = &quot;batlow&quot;, begin=0.2, end=0.8)+ theme_few()+ theme(legend.position = &quot;top&quot;)+ labs(x= &quot;buffer distance (m)&quot;) # save ggsave(fig_spatial_thinning, filename = &quot;figs/fig_spatial_thinning_02.png&quot;, width = 10, height = 4, dpi=300); dev.off() "]
]
